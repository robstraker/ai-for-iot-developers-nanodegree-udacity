#!/usr/bin/env python
# coding: utf-8

# # Multi Device Plugin and the DevCloud
# 
# In this section, we will try to load a model on to three devices at the same time: an NCS2, a GPU and a CPU and calculate the time it takes to do the same.

# 
# 
# #### Set up paths so we can run Dev Cloud utilities
# You *must* run this every time they enter a Workspace session.

# In[7]:


get_ipython().run_line_magic('env', 'PATH=/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/intel_devcloud_support')
import os
import sys
sys.path.insert(0, os.path.abspath('/opt/intel_devcloud_support'))
sys.path.insert(0, os.path.abspath('/opt/intel'))


# ## The model
# 
# We will be using the `vehicle-license-plate-detection-barrier-0106` model for this exercise. Remember that to run a model on the GPU and NCS2, we need to use FP16 as the model precision. Even though, to run a model on the CPU, it is prefered to use FP32, we will be using FP16.
# 
# The model is present in the `/data/models/intel` folder.

# # Step 1: Creating a Python Script
# 
# The first step is to create a python script that you can use to load the model and perform an inference.
# 
# The advantage of using the Multi device plugin is that it does not require us to change our application code. So we will be using the same python script as before.

# In[8]:


get_ipython().run_cell_magic('writefile', 'load_model_to_device.py', '\nimport time\nfrom openvino.inference_engine import IENetwork\nfrom openvino.inference_engine import IEPlugin\nimport argparse\n\ndef main(args):\n    model=args.model_path\n    model_weights=model+\'.bin\'\n    model_structure=model+\'.xml\'\n    \n    start=time.time()\n    model=IENetwork(model_structure, model_weights)\n\n    plugin = IEPlugin(device=args.device)\n    \n    net = plugin.load(network=model, num_requests=1)\n    print(f"Time taken to load model = {time.time()-start} seconds")\n\nif __name__==\'__main__\':\n    parser=argparse.ArgumentParser()\n    parser.add_argument(\'--model_path\', required=True)\n    parser.add_argument(\'--device\', default=None)\n    \n    args=parser.parse_args() \n    main(args)')


# ## Step 2: Creating a job submission script
# 
# To submit a job to the devcloud, we need to create a script. I have named the script as `inference_multi_model_job.sh`. We need to pass two variables in this script: the path to the model and the device we want to load our model on.
# 
# Just like the python script, our job submission script also does not need to change. The only change will be how we specify the device when submitting the job.

# In[9]:


get_ipython().run_cell_magic('writefile', 'load_multi_model_job.sh', '# Here we use the writefile magic command to write a file to the directory\n# The file we are writing is the job submission script\n\n#Get the command line arguments\n\nDEVICE=$1\nMODELPATH=$2\n\n# Run the load model python script\npython3 load_model_to_device.py  --model_path ${MODELPATH} --device ${DEVICE}')


# ## Step 3: Submitting a Multi Device Plugin Job
# 
# In this case, we need to request three devices instead of two. The tank-870 edge node contains all the three devices, so we will specify that, followed by the three devices.
# 
# This time along with specifying the model path, we also need to specify the device which is going to be `MULTI:NCS,GPU,CPU`. Also, remember that the model precision for running is going to be FP16.

# In[10]:


job_id_core = get_ipython().getoutput('/home/workspace/qsub load_multi_model_job.sh -d . nodes=1:tank-870:i5-6500te:intel-hd-530:intel-ncs2 -F "MULTI:MYRIAD,GPU,CPU /data/models/intel/vehicle-license-plate-detection-barrier-0106/FP16/vehicle-license-plate-detection-barrier-0106" -N store_core ')
print(job_id_core[0])


# ## Step 4: Getting the Live Stat Values
# 
# By running the below command, we can see the live status of our job

# In[11]:


import liveQStat
liveQStat.liveQstat()


# ## Step 5: View the Outputs

# In[15]:


import get_results

#get_results.getResults(job_id_core[0], filename="retail.tgz", blocking=True)
get_results.getResults(job_id_core[0], get_stderr=True, blocking=True)
# get_results.getResults(job_id_core[0], blocking=True)


# In[ ]:




