{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_qwukc5b"
   },
   "source": [
    "# Exercise: Heterogenous Plugin and the DevCloud\n",
    "\n",
    "In this exercise, we will load a model using the hetero plugin on to the FPGA and CPU, and the GPU and CPU. We will then perform an inference on it and compare the time it takes to do the same for each device pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_z8bfs11"
   },
   "source": [
    "<span class=\"graffiti-highlight graffiti-id_z8bfs11-id_d97ox8f\"><i></i><button>Graffiti Sample Button (edit me)</button></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_0untint"
   },
   "source": [
    "\n",
    "\n",
    "#### Set up paths so we can run Dev Cloud utilities\n",
    "You *must* run this every time they enter a Workspace session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "graffitiCellId": "id_axn1sb2"
   },
   "outputs": [],
   "source": [
    "%env PATH=/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/intel_devcloud_support\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('/opt/intel_devcloud_support'))\n",
    "sys.path.insert(0, os.path.abspath('/opt/intel'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_mhiayyz"
   },
   "source": [
    "## The model\n",
    "\n",
    "We will be using the `vehicle-license-plate-detection-barrier-0106` model for this exercise. Remember that to run a model using the HETERO Plugin, we need to use FP16 as the model precision.\n",
    "\n",
    "The model is present in the `/data/models/intel` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_ltf95ei"
   },
   "source": [
    "# Step 1: Creating a Python Script\n",
    "\n",
    "The first step is to create a python script that you can use to load the model and perform an inference. I have used the `writefile` magic to create a python file called `inference_on_device.py`. You will need to complete this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "graffitiCellId": "id_bpywo8s"
   },
   "outputs": [],
   "source": [
    "%%writefile inference_on_device.py\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "from openvino.inference_engine import IENetwork\n",
    "from openvino.inference_engine import IECore\n",
    "import argparse\n",
    "\n",
    "def main(args):\n",
    "    model=args.model_path\n",
    "    model_weights=model+'.bin'\n",
    "    model_structure=model+'.xml'\n",
    "    \n",
    "    start=time.time()\n",
    "    \n",
    "    # TODO: Load the model on VPU\n",
    "    \n",
    "    print(f\"Time taken to load model = {time.time()-start} seconds\")\n",
    "    \n",
    "    # Reading and Preprocessing Image\n",
    "    input_img=cv2.imread('car.png')\n",
    "    input_img=cv2.resize(input_img, (300,300), interpolation = cv2.INTER_AREA)\n",
    "    input_img=np.moveaxis(input_img, -1, 0)\n",
    "\n",
    "    # TODO: Prepare the model for inference (create input dict etc.)\n",
    "    \n",
    "    start=time.time()\n",
    "    for _ in range(100):\n",
    "        # TODO: Run Inference in a Loop\n",
    "    \n",
    "    print(f\"Time Taken to run 100 Inference is = {time.time()-start} seconds\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser=argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_path', required=True)\n",
    "    parser.add_argument('--device', default=None)\n",
    "    \n",
    "    args=parser.parse_args() \n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_1rnmf5g"
   },
   "source": [
    "<span class=\"graffiti-highlight graffiti-id_1rnmf5g-id_nmeqj1a\"><i></i><button>Hide Solution</button></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "graffitiCellId": "id_nmeqj1a"
   },
   "outputs": [],
   "source": [
    "%%writefile inference_on_device.py\n",
    "\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "from openvino.inference_engine import IENetwork\n",
    "from openvino.inference_engine import IECore\n",
    "import argparse\n",
    "\n",
    "def main(args):\n",
    "    model=args.model_path\n",
    "    model_weights=model+'.bin'\n",
    "    model_structure=model+'.xml'\n",
    "    \n",
    "    start=time.time()\n",
    "    model=IENetwork(model_structure, model_weights)\n",
    "\n",
    "    core = IECore()\n",
    "    net = core.load_network(network=model, device_name=args.device, num_requests=1)\n",
    "    load_time=time.time()-start\n",
    "    print(f\"Time taken to load model = {load_time} seconds\")\n",
    "    \n",
    "    # Get the name of the input node\n",
    "    input_name=next(iter(model.inputs))\n",
    "\n",
    "    # Reading and Preprocessing Image\n",
    "    input_img=cv2.imread('/data/resources/car.png')\n",
    "    input_img=cv2.resize(input_img, (300,300), interpolation = cv2.INTER_AREA)\n",
    "    input_img=np.moveaxis(input_img, -1, 0)\n",
    "\n",
    "    # Running Inference in a loop on the same image\n",
    "    input_dict={input_name:input_img}\n",
    "\n",
    "    start=time.time()\n",
    "    for _ in range(100):\n",
    "        net.infer(input_dict)\n",
    "    \n",
    "    inference_time=time.time()-start\n",
    "    fps=100/inference_time\n",
    "    \n",
    "    print(f\"Time Taken to run 100 Inference is = {inference_time} seconds\")\n",
    "    \n",
    "    with open(f\"/output/{args.path}.txt\", \"w\") as f:\n",
    "        f.write(str(load_time)+'\\n')\n",
    "        f.write(str(inference_time)+'\\n')\n",
    "        f.write(str(fps)+'\\n')\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser=argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_path', required=True)\n",
    "    parser.add_argument('--device', default=None)\n",
    "    parser.add_argument('--path', default=None)\n",
    "    \n",
    "    args=parser.parse_args() \n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_ufbi2ll"
   },
   "source": [
    "## Step 2: Creating a job submission script\n",
    "\n",
    "To submit a job to the devcloud, we need to create a script. I have named the script as `inference_hetero_model_job.sh`.\n",
    "\n",
    "Can you write a script that will take the model path and device as a command line argument and then call the python file you created in the previous cell with the path to the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "graffitiCellId": "id_5r13clu"
   },
   "outputs": [],
   "source": [
    "%%writefile inference_model_job.sh\n",
    "\n",
    "#TODO: Create job submission script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_f1nbmn9"
   },
   "source": [
    "<span class=\"graffiti-highlight graffiti-id_f1nbmn9-id_ia7yjlq\"><i></i><button>Hide Solution</button></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "graffitiCellId": "id_ia7yjlq"
   },
   "outputs": [],
   "source": [
    "%%writefile inference_model_job.sh\n",
    "#!/bin/bash\n",
    "\n",
    "exec 1>/output/stdout.log 2>/output/stderr.log\n",
    "\n",
    "mkdir -p /output\n",
    "\n",
    "DEVICE=$1\n",
    "MODELPATH=$2\n",
    "SAVEPATH=$3\n",
    "\n",
    "\n",
    "source /opt/intel/init_openvino.sh\n",
    "aocl program acl0 /opt/intel/openvino/bitstreams/a10_vision_design_sg1_bitstreams/2019R4_PL1_FP16_MobileNet_Clamp.aocx\n",
    "\n",
    "\n",
    "# Run the load model python script\n",
    "python3 inference_on_device.py  --model_path ${MODELPATH} --device ${DEVICE} --path ${SAVEPATH}\n",
    "\n",
    "cd /output\n",
    "\n",
    "tar zcvf output.tgz *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_28fed2h"
   },
   "source": [
    "## Step 3a: Running on the FPGA and CPU\n",
    "\n",
    "In the cell below, can you write the qsub command that will submit your job to the CPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "graffitiCellId": "id_6awpacu"
   },
   "outputs": [],
   "source": [
    "fpga_cpu_job = # TODO: Write qsub command\n",
    "print(fpga_cpu_job[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_cvp3lyi"
   },
   "source": [
    "<span class=\"graffiti-highlight graffiti-id_cvp3lyi-id_chmeh50\"><i></i><button>Hide Solution</button></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "graffitiCellId": "id_chmeh50"
   },
   "outputs": [],
   "source": [
    "fpga_cpu_job = !qsub inference_model_job.sh -d . -l nodes=1:tank-870:i5-6500te:iei-mustang-f100-a10 -F \"HETERO:FPGA,CPU /data/models/intel/vehicle-license-plate-detection-barrier-0106/FP16/vehicle-license-plate-detection-barrier-0106 fpga_cpu_stats\" -N store_core \n",
    "print(fpga_cpu_job[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_io25c53"
   },
   "source": [
    "## Step 3b: Running on GPU and CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "graffitiCellId": "id_v5klpi1"
   },
   "outputs": [],
   "source": [
    "gpu_cpu_job = # TODO: Write qsub command\n",
    "print(gpu_cpu_job[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_7k34s6u"
   },
   "source": [
    "<span class=\"graffiti-highlight graffiti-id_7k34s6u-id_022l4bj\"><i></i><button>Hide Solution</button></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "graffitiCellId": "id_022l4bj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S2lH4HdsYcOlcJglYUJLUPfklDZlfkpL\n"
     ]
    }
   ],
   "source": [
    "gpu_cpu_job = !qsub inference_model_job.sh -d . -l nodes=tank-870:i5-6500te:intel-hd-530 -F \"HETERO:GPU,CPU /data/models/intel/vehicle-license-plate-detection-barrier-0106/FP16/vehicle-license-plate-detection-barrier-0106 gpu_cpu_stats\" -N store_core \n",
    "print(gpu_cpu_job[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_io25c53"
   },
   "source": [
    "## Step 3c: Running on FPGA, GPU and CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "graffitiCellId": "id_v5klpi1"
   },
   "outputs": [],
   "source": [
    "fpga_gpu_cpu_job = # TODO: Write qsub command\n",
    "print(fpga_gpu_cpu_job[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_mxh5ozv"
   },
   "source": [
    "<span class=\"graffiti-highlight graffiti-id_mxh5ozv-id_qicoukm\"><i></i><button>Hide Solution</button></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "graffitiCellId": "id_qicoukm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WiJxEaE9GUAiBpaxN24mRigPOtNKHwjy\n"
     ]
    }
   ],
   "source": [
    "fpga_gpu_cpu_job = !qsub inference_model_job.sh -d . -l nodes=tank-870:i5-6500te:intel-hd-530:iei-mustang-f100-a10 -F \"HETERO:FPGA,GPU,CPU /data/models/intel/vehicle-license-plate-detection-barrier-0106/FP16/vehicle-license-plate-detection-barrier-0106 fpga_gpu_cpu_stats\" -N store_core \n",
    "print(fpga_gpu_cpu_job[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_8ym2smn"
   },
   "source": [
    "## Step 4: Getting the Live Stat Values\n",
    "\n",
    "By running the below command, we can see the live status of the commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_clj7fxa"
   },
   "source": [
    "<span class=\"graffiti-highlight graffiti-id_clj7fxa-id_d3gqjz0\"><i></i><button>Graffiti Sample Button (edit me)</button></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "graffitiCellId": "id_zig7qg5"
   },
   "outputs": [],
   "source": [
    "import liveQStat\n",
    "liveQStat.liveQStat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_2vp5y4m"
   },
   "source": [
    "## Step 5a: Get the results for FPGA and CPU\n",
    "\n",
    "Running the cell below will get the output files from our job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_cygruth"
   },
   "source": [
    "<span class=\"graffiti-highlight graffiti-id_cygruth-id_6nd1x96\"><i></i><button>Graffiti Sample Button (edit me)</button></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "graffitiCellId": "id_zpdshwo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getResults() is blocking until results of the job (id:3wbYvIKVxiiCcGAmrnbbowTaA7tHRjuv) are ready.\n",
      "Please wait...Success!\n",
      "output.tgz was downloaded in the same folder as this notebook.\n"
     ]
    }
   ],
   "source": [
    "import get_results\n",
    "\n",
    "get_results.getResults(fpga_cpu_job[0], get_stderr=True, filename=\"output.tgz\", blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "graffitiCellId": "id_0quk13q"
   },
   "outputs": [],
   "source": [
    "!tar zxf output.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "graffitiCellId": "id_l1gs5j5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTELFPGAOCLSDKROOT is set to /opt/altera/aocl-pro-rte/aclrte-linux64. Using that.\r\n",
      "\r\n",
      "aoc was not found, but aocl was found. Assuming only RTE is installed.\r\n",
      "\r\n",
      "AOCL_BOARD_PACKAGE_ROOT is set to /opt/intel/openvino/bitstreams/a10_vision_design_sg1_bitstreams/BSP/a10_1150_sg1. Using that.\r\n",
      "Adding /opt/altera/aocl-pro-rte/aclrte-linux64/bin to PATH\r\n",
      "Adding /opt/altera/aocl-pro-rte/aclrte-linux64/host/linux64/lib to LD_LIBRARY_PATH\r\n",
      "Adding /opt/intel/openvino/bitstreams/a10_vision_design_sg1_bitstreams/BSP/a10_1150_sg1/linux64/lib to LD_LIBRARY_PATH\r\n",
      "[setupvars.sh] OpenVINO environment initialized\r\n",
      "aocl program: Running program from /opt/intel/openvino/bitstreams/a10_vision_design_sg1_bitstreams/BSP/a10_1150_sg1/linux64/libexec\r\n",
      "Programming device: a10gx_2ddr : Intel Vision Accelerator Design with Intel Arria 10 FPGA (acla10_1150_sg10)\r\n",
      "Program succeed. \r\n",
      "Time taken to load model = 3.989006996154785 seconds\r\n",
      "Time Taken to run 100 Inference is = 0.8590676784515381 seconds\r\n",
      "fpga_cpu_stats.txt\r\n",
      "stderr.log\r\n"
     ]
    }
   ],
   "source": [
    "!cat stdout.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_gykmtow"
   },
   "source": [
    "## Step 5b: Get the result for GPU and CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "graffitiCellId": "id_8w79u8w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getResults() is blocking until results of the job (id:S2lH4HdsYcOlcJglYUJLUPfklDZlfkpL) are ready.\n",
      "Please wait...Success!\n",
      "output.tgz was downloaded in the same folder as this notebook.\n"
     ]
    }
   ],
   "source": [
    "import get_results\n",
    "\n",
    "get_results.getResults(gpu_cpu_job[0], filename=\"output.tgz\", blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "graffitiCellId": "id_kv4qcd6"
   },
   "outputs": [],
   "source": [
    "!tar zxf output.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "graffitiCellId": "id_etgr4le"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DetectionOutput_Reshape_priors_/Output_0/Data__const is CPU\r\n",
      "DetectionOutput_Reshape_conf_ is CPU\r\n",
      "SSD/concat_reshape_softmax/mbox_conf_final is CPU\r\n",
      "SSD/concat_reshape_softmax/Reshape is GPU\r\n",
      "SSD/concat_reshape_softmax/mbox_conf_logits is GPU\r\n",
      "SSD/ssd_head_1/Flatten_1/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head_1/layer_18/output_mbox_conf/BiasAdd/Add/Transpose is GPU\r\n",
      "DetectionOutput_Reshape_loc_ is CPU\r\n",
      "SSD/concat_reshape_softmax/mbox_loc_final is CPU\r\n",
      "SSD/ssd_head_1/Flatten/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head_1/layer_18/output_mbox_loc/BiasAdd/Add/Transpose is GPU\r\n",
      "SSD/ssd_head_2/Flatten_1/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head_2/feature_map_1_mbox_conf/BiasAdd/Add/Transpose is GPU\r\n",
      "SSD/ssd_head_2/Flatten/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head_2/feature_map_1_mbox_loc/BiasAdd/Add/Transpose is GPU\r\n",
      "SSD/ssd_head_3/Flatten_1/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head_3/feature_map_2_mbox_conf/BiasAdd/Add/Transpose is GPU\r\n",
      "SSD/ssd_head_3/Flatten/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head_3/feature_map_2_mbox_loc/BiasAdd/Add/Transpose is GPU\r\n",
      "SSD/ssd_head_4/Flatten_1/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head_4/feature_map_3_mbox_conf/BiasAdd/Add/Transpose is GPU\r\n",
      "SSD/ssd_head_4/Flatten/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head_4/feature_map_3_mbox_loc/BiasAdd/Add/Transpose is GPU\r\n",
      "SSD/ssd_head_5/Flatten_1/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head_5/feature_map_4_mbox_conf/BiasAdd/Add/Transpose is GPU\r\n",
      "SSD/ssd_head_5/Flatten/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head_5/feature_map_4_mbox_loc/BiasAdd/Add/Transpose is GPU\r\n",
      "SSD/ssd_head/Flatten_1/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head/layer_14/output_mbox_conf/BiasAdd/Add/Transpose is GPU\r\n",
      "SSD/ssd_head/Flatten/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head/layer_14/output_mbox_loc/BiasAdd/Add/Transpose is GPU\r\n",
      "Time taken to load model = 26.8349506855011 seconds\r\n",
      "Time Taken to run 100 Inference is = 0.6559219360351562 seconds\r\n",
      "gpu_cpu_stats.txt\r\n",
      "stderr.log\r\n"
     ]
    }
   ],
   "source": [
    "!cat stdout.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_gykmtow"
   },
   "source": [
    "## Step 5c: Get the result for FPGA, GPU and CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "graffitiCellId": "id_8w79u8w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getResults() is blocking until results of the job (id:WiJxEaE9GUAiBpaxN24mRigPOtNKHwjy) are ready.\n",
      "Please wait...Success!\n",
      "output.tgz was downloaded in the same folder as this notebook.\n"
     ]
    }
   ],
   "source": [
    "import get_results\n",
    "\n",
    "get_results.getResults(fpga_gpu_cpu_job[0], filename=\"output.tgz\", blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "graffitiCellId": "id_kv4qcd6"
   },
   "outputs": [],
   "source": [
    "!tar zxf output.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "graffitiCellId": "id_etgr4le"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTELFPGAOCLSDKROOT is set to /opt/altera/aocl-pro-rte/aclrte-linux64. Using that.\r\n",
      "\r\n",
      "aoc was not found, but aocl was found. Assuming only RTE is installed.\r\n",
      "\r\n",
      "AOCL_BOARD_PACKAGE_ROOT is set to /opt/intel/openvino/bitstreams/a10_vision_design_sg1_bitstreams/BSP/a10_1150_sg1. Using that.\r\n",
      "Adding /opt/altera/aocl-pro-rte/aclrte-linux64/bin to PATH\r\n",
      "Adding /opt/altera/aocl-pro-rte/aclrte-linux64/host/linux64/lib to LD_LIBRARY_PATH\r\n",
      "Adding /opt/intel/openvino/bitstreams/a10_vision_design_sg1_bitstreams/BSP/a10_1150_sg1/linux64/lib to LD_LIBRARY_PATH\r\n",
      "[setupvars.sh] OpenVINO environment initialized\r\n",
      "aocl program: Running program from /opt/intel/openvino/bitstreams/a10_vision_design_sg1_bitstreams/BSP/a10_1150_sg1/linux64/libexec\r\n",
      "Programming device: a10gx_2ddr : Intel Vision Accelerator Design with Intel Arria 10 FPGA (acla10_1150_sg10)\r\n",
      "Program succeed. \r\n",
      "DetectionOutput_Reshape_priors_/Output_0/Data__const is CPU\r\n",
      "DetectionOutput_Reshape_conf_ is CPU\r\n",
      "SSD/concat_reshape_softmax/mbox_conf_final is CPU\r\n",
      "SSD/concat_reshape_softmax/Reshape is GPU\r\n",
      "SSD/concat_reshape_softmax/mbox_conf_logits is GPU\r\n",
      "SSD/ssd_head_1/Flatten_1/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head_1/layer_18/output_mbox_conf/BiasAdd/Add/Transpose is GPU\r\n",
      "DetectionOutput_Reshape_loc_ is CPU\r\n",
      "SSD/concat_reshape_softmax/mbox_loc_final is CPU\r\n",
      "SSD/ssd_head_1/Flatten/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head_1/layer_18/output_mbox_loc/BiasAdd/Add/Transpose is GPU\r\n",
      "SSD/ssd_head_2/Flatten_1/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head_2/feature_map_1_mbox_conf/BiasAdd/Add/Transpose is GPU\r\n",
      "SSD/ssd_head_2/Flatten/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head_2/feature_map_1_mbox_loc/BiasAdd/Add/Transpose is GPU\r\n",
      "SSD/ssd_head_3/Flatten_1/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head_3/feature_map_2_mbox_conf/BiasAdd/Add/Transpose is GPU\r\n",
      "SSD/ssd_head_3/Flatten/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head_3/feature_map_2_mbox_loc/BiasAdd/Add/Transpose is GPU\r\n",
      "SSD/ssd_head_4/Flatten_1/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head_4/feature_map_3_mbox_conf/BiasAdd/Add/Transpose is GPU\r\n",
      "SSD/ssd_head_4/Flatten/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head_4/feature_map_3_mbox_loc/BiasAdd/Add/Transpose is GPU\r\n",
      "SSD/ssd_head_5/Flatten_1/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head_5/feature_map_4_mbox_conf/BiasAdd/Add/Transpose is GPU\r\n",
      "SSD/ssd_head_5/Flatten/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head_5/feature_map_4_mbox_loc/BiasAdd/Add/Transpose is GPU\r\n",
      "SSD/ssd_head/Flatten_1/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head/layer_14/output_mbox_conf/BiasAdd/Add/Transpose is GPU\r\n",
      "SSD/ssd_head/Flatten/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head/layer_14/output_mbox_loc/BiasAdd/Add/Transpose is GPU\r\n",
      "Time taken to load model = 11.313783645629883 seconds\r\n",
      "Time Taken to run 100 Inference is = 1.3228466510772705 seconds\r\n",
      "fpga_gpu_cpu_stats.txt\r\n",
      "stderr.log\r\n"
     ]
    }
   ],
   "source": [
    "!cat stdout.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_4rf323l"
   },
   "source": [
    "## Step 6: View the Outputs\n",
    "\n",
    "Can you plot the load time, inference time and the frames per second in the cell below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "graffitiCellId": "id_bkny5ta"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#File Paths to stats files\n",
    "paths=[]\n",
    "\n",
    "# TODO: Plot the different stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_m9kxw9k"
   },
   "source": [
    "<span class=\"graffiti-highlight graffiti-id_m9kxw9k-id_4h5tl2h\"><i></i><button>Hide Solution</button></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "graffitiCellId": "id_4h5tl2h"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(labels, data, title, label):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "    ax.set_ylabel(label)\n",
    "    ax.set_title(title)\n",
    "    ax.bar(labels, data)\n",
    "    \n",
    "def read_files(paths, labels):\n",
    "    load_time=[]\n",
    "    inference_time=[]\n",
    "    fps=[]\n",
    "    \n",
    "    for path in paths:\n",
    "        if os.path.isfile(path):\n",
    "            f=open(path, 'r')\n",
    "            load_time.append(float(f.readline()))\n",
    "            inference_time.append(float(f.readline()))\n",
    "            fps.append(float(f.readline()))\n",
    "\n",
    "    plot(labels, load_time, 'Model Load Time', 'seconds')\n",
    "    plot(labels, inference_time, 'Inference Time', 'seconds')\n",
    "    plot(labels, fps, 'Frames per Second', 'Frames')\n",
    "\n",
    "paths=['fpga_cpu_stats.txt', 'gpu_cpu_stats.txt', 'fpga_gpu_cpu_stats.txt']\n",
    "read_files(paths, ['FPGA/CPU', 'GPU/CPU', 'FPGA/GPU/CPU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "graffitiCellId": "id_0c20r8n"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "graffiti": {
   "firstAuthorId": "dca260a8-2142-11ea-b0f7-6f7abbbf2f85",
   "id": "id_610hfgn",
   "language": "EN"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
