{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Device Plugin and the DevCloud\n",
    "\n",
    "In this section, we will try to load a model on to three devices at the same time: an NCS2, a GPU and a CPU and calculate the time it takes to do the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Set up paths so we can run Dev Cloud utilities\n",
    "You *must* run this every time they enter a Workspace session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/intel_devcloud_support\n"
     ]
    }
   ],
   "source": [
    "%env PATH=/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/intel_devcloud_support\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('/opt/intel_devcloud_support'))\n",
    "sys.path.insert(0, os.path.abspath('/opt/intel'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "We will be using the `vehicle-license-plate-detection-barrier-0106` model for this exercise. Remember that to run a model on the GPU and NCS2, we need to use FP16 as the model precision. Even though, to run a model on the CPU, it is prefered to use FP32, we will be using FP16.\n",
    "\n",
    "The model is present in the `/data/models/intel` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Creating a Python Script\n",
    "\n",
    "The first step is to create a python script that you can use to load the model and perform an inference.\n",
    "\n",
    "The advantage of using the Multi device plugin is that it does not require us to change our application code. So we will be using the same python script as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference_cpu_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile load_model_to_device.py\n",
    "\n",
    "import time\n",
    "from openvino.inference_engine import IENetwork\n",
    "from openvino.inference_engine import IEPlugin\n",
    "import argparse\n",
    "\n",
    "def main(args):\n",
    "    model=args.model_path\n",
    "    model_weights=model+'.bin'\n",
    "    model_structure=model+'.xml'\n",
    "    \n",
    "    start=time.time()\n",
    "    model=IENetwork(model_structure, model_weights)\n",
    "\n",
    "    plugin = IEPlugin(device=args.device)\n",
    "    \n",
    "    net = plugin.load(network=model, num_requests=1)\n",
    "    print(f\"Time taken to load model = {time.time()-start} seconds\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser=argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_path', required=True)\n",
    "    parser.add_argument('--device', default=None)\n",
    "    \n",
    "    args=parser.parse_args() \n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Creating a job submission script\n",
    "\n",
    "To submit a job to the devcloud, we need to create a script. I have named the script as `inference_multi_model_job.sh`. We need to pass two variables in this script: the path to the model and the device we want to load our model on.\n",
    "\n",
    "Just like the python script, our job submission script also does not need to change. The only change will be how we specify the device when submitting the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference_cpu_model_job.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile load_multi_model_job.sh\n",
    "# Here we use the writefile magic command to write a file to the directory\n",
    "# The file we are writing is the job submission script\n",
    "\n",
    "#Get the command line arguments\n",
    "\n",
    "DEVICE=$1\n",
    "MODELPATH=$2\n",
    "\n",
    "# Run the load model python script\n",
    "python3 load_model_to_device.py  --model_path ${MODELPATH} --device ${DEVICE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Submitting a Multi Device Plugin Job\n",
    "\n",
    "In this case, we need to request three devices instead of two. The tank-870 edge node contains all the three devices, so we will specify that, followed by the three devices.\n",
    "\n",
    "This time along with specifying the model path, we also need to specify the device which is going to be `MULTI:NCS,GPU,CPU`. Also, remember that the model precision for running is going to be FP16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xaEy1kJNC8lCBJ11tE41AxkALxjPzHRb\n"
     ]
    }
   ],
   "source": [
    "job_id_core = !/home/workspace/qsub load_multi_model_job.sh -d . nodes=1:tank-870:i5-6500te:intel-hd-530:intel-ncs2 -F \"MULTI:MYRIAD,GPU,CPU /data/models/intel/vehicle-license-plate-detection-barrier-0106/FP16/vehicle-license-plate-detection-barrier-0106\" -N store_core \n",
    "print(job_id_core[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Getting the Live Stat Values\n",
    "\n",
    "By running the below command, we can see the live status of our job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'liveQStat' has no attribute 'liveQstat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6afc868a2b9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mliveQStat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mliveQStat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliveQstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'liveQStat' has no attribute 'liveQstat'"
     ]
    }
   ],
   "source": [
    "import liveQStat\n",
    "liveQStat.liveQstat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: View the Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getResults() is blocking until results of the job are ready.\n",
      "Please wait...\n",
      "Traceback (most recent call last):\n",
      "  File \"load_model_to_cpu.py\", line 25, in <module>\n",
      "    main(args)\n",
      "  File \"load_model_to_cpu.py\", line 13, in main\n",
      "    model=IENetwork(model_structure, model_weights)\n",
      "  File \"ie_api.pyx\", line 975, in openvino.inference_engine.ie_api.IENetwork.__cinit__\n",
      "Exception: Path to the model /data/models/intel/vehicle-license-plate-detection-barrier-0106/FP16/vehicle-license-plate-detection-barrier-0106.xml doesn't exists or it's a directory\n",
      "rm: cannot remove '/tmp/docker-39836/docker/libnetwork/10289b456d34.sock': Permission denied\n",
      "rm: cannot remove '/tmp/docker-39836/docker/libnetwork/7f5deeddc04b.sock': Permission denied\n",
      "rm: cannot remove '/tmp/docker-39836/docker/libnetwork/b006c46ad247.sock': Permission denied\n",
      "rm: cannot remove '/tmp/docker-39836/docker/libnetwork/9c51b1cfe8e4.sock': Permission denied\n",
      "rm: cannot remove '/tmp/docker-39836/docker/libnetwork/7fd7e39e7ffc.sock': Permission denied\n",
      "rm: cannot remove '/tmp/docker-39836/docker/libnetwork/e8af8d968899.sock': Permission denied\n",
      "rm: cannot remove '/tmp/docker-39836/docker/libnetwork/77b78485be08.sock': Permission denied\n",
      "rm: cannot remove '/tmp/docker-39836/docker/libnetwork/faac716f875c.sock': Permission denied\n",
      "rm: cannot remove '/tmp/docker-39836/docker/libnetwork/d2e6aad93058.sock': Permission denied\n",
      "rm: cannot remove '/tmp/docker-39836/docker/libnetwork/76b6b6098937.sock': Permission denied\n",
      "rm: cannot remove '/tmp/docker-39836/docker/libnetwork/273a4b887bee.sock': Permission denied\n",
      "rm: cannot remove '/tmp/docker-39836/docker/libnetwork/c963960eae8c.sock': Permission denied\n",
      "rm: cannot remove '/tmp/docker-39836/docker/libnetwork/09e22900dcea.sock': Permission denied\n",
      "rm: cannot remove '/tmp/docker-39836/docker/libnetwork/5f3d3e6a14a1.sock': Permission denied\n",
      "rm: cannot remove '/tmp/docker-39836/docker/libnetwork/48a3473775e2.sock': Permission denied\n",
      "rm: cannot remove '/tmp/docker-39836/docker/libnetwork/bdcf665f589f.sock': Permission denied\n",
      "rm: cannot remove '/tmp/docker-39836/docker/libnetwork/f5097a94bc7f.sock': Permission denied\n",
      "rm: cannot remove '/tmp/docker-39836/docker/libnetwork/a8963e1c829d.sock': Permission denied\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import get_results\n",
    "\n",
    "#get_results.getResults(job_id_core[0], filename=\"retail.tgz\", blocking=True)\n",
    "get_results.getResults(job_id_core[0], get_stderr=True, blocking=True)\n",
    "# get_results.getResults(job_id_core[0], blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
