#!/usr/bin/env python
# coding: utf-8

# # Exercise: Heterogenous Plugin and the DevCloud
# 
# In this exercise, we will load a model using the hetero plugin on to the FPGA and CPU, and the GPU and CPU. We will then perform an inference on it and compare the time it takes to do the same for each device pair.

# <span class="graffiti-highlight graffiti-id_z8bfs11-id_d97ox8f"><i></i><button>Graffiti Sample Button (edit me)</button></span>

# 
# 
# #### Set up paths so we can run Dev Cloud utilities
# You *must* run this every time they enter a Workspace session.

# In[ ]:


get_ipython().run_line_magic('env', 'PATH=/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/intel_devcloud_support')
import os
import sys
sys.path.insert(0, os.path.abspath('/opt/intel_devcloud_support'))
sys.path.insert(0, os.path.abspath('/opt/intel'))


# ## The model
# 
# We will be using the `vehicle-license-plate-detection-barrier-0106` model for this exercise. Remember that to run a model using the HETERO Plugin, we need to use FP16 as the model precision.
# 
# The model is present in the `/data/models/intel` folder.

# # Step 1: Creating a Python Script
# 
# The first step is to create a python script that you can use to load the model and perform an inference. I have used the `writefile` magic to create a python file called `inference_on_device.py`. You will need to complete this file.

# In[ ]:


get_ipython().run_cell_magic('writefile', 'inference_on_device.py', '\nimport time\nimport numpy as np\nimport cv2\nfrom openvino.inference_engine import IENetwork\nfrom openvino.inference_engine import IECore\nimport argparse\n\ndef main(args):\n    model=args.model_path\n    model_weights=model+\'.bin\'\n    model_structure=model+\'.xml\'\n    \n    start=time.time()\n    \n    # TODO: Load the model on VPU\n    \n    print(f"Time taken to load model = {time.time()-start} seconds")\n    \n    # Reading and Preprocessing Image\n    input_img=cv2.imread(\'car.png\')\n    input_img=cv2.resize(input_img, (300,300), interpolation = cv2.INTER_AREA)\n    input_img=np.moveaxis(input_img, -1, 0)\n\n    # TODO: Prepare the model for inference (create input dict etc.)\n    \n    start=time.time()\n    for _ in range(100):\n        # TODO: Run Inference in a Loop\n    \n    print(f"Time Taken to run 100 Inference is = {time.time()-start} seconds")\n\nif __name__==\'__main__\':\n    parser=argparse.ArgumentParser()\n    parser.add_argument(\'--model_path\', required=True)\n    parser.add_argument(\'--device\', default=None)\n    \n    args=parser.parse_args() \n    main(args)')


# <span class="graffiti-highlight graffiti-id_1rnmf5g-id_nmeqj1a"><i></i><button>Hide Solution</button></span>

# In[ ]:


get_ipython().run_cell_magic('writefile', 'inference_on_device.py', '\nimport time\nimport cv2\nimport numpy as np\nfrom openvino.inference_engine import IENetwork\nfrom openvino.inference_engine import IECore\nimport argparse\n\ndef main(args):\n    model=args.model_path\n    model_weights=model+\'.bin\'\n    model_structure=model+\'.xml\'\n    \n    start=time.time()\n    model=IENetwork(model_structure, model_weights)\n\n    core = IECore()\n    net = core.load_network(network=model, device_name=args.device, num_requests=1)\n    load_time=time.time()-start\n    print(f"Time taken to load model = {load_time} seconds")\n    \n    # Get the name of the input node\n    input_name=next(iter(model.inputs))\n\n    # Reading and Preprocessing Image\n    input_img=cv2.imread(\'/data/resources/car.png\')\n    input_img=cv2.resize(input_img, (300,300), interpolation = cv2.INTER_AREA)\n    input_img=np.moveaxis(input_img, -1, 0)\n\n    # Running Inference in a loop on the same image\n    input_dict={input_name:input_img}\n\n    start=time.time()\n    for _ in range(100):\n        net.infer(input_dict)\n    \n    inference_time=time.time()-start\n    fps=100/inference_time\n    \n    print(f"Time Taken to run 100 Inference is = {inference_time} seconds")\n    \n    with open(f"/output/{args.path}.txt", "w") as f:\n        f.write(str(load_time)+\'\\n\')\n        f.write(str(inference_time)+\'\\n\')\n        f.write(str(fps)+\'\\n\')\n\nif __name__==\'__main__\':\n    parser=argparse.ArgumentParser()\n    parser.add_argument(\'--model_path\', required=True)\n    parser.add_argument(\'--device\', default=None)\n    parser.add_argument(\'--path\', default=None)\n    \n    args=parser.parse_args() \n    main(args)')


# ## Step 2: Creating a job submission script
# 
# To submit a job to the devcloud, we need to create a script. I have named the script as `inference_hetero_model_job.sh`.
# 
# Can you write a script that will take the model path and device as a command line argument and then call the python file you created in the previous cell with the path to the model?

# In[ ]:


get_ipython().run_cell_magic('writefile', 'inference_model_job.sh', '\n#TODO: Create job submission script')


# <span class="graffiti-highlight graffiti-id_f1nbmn9-id_ia7yjlq"><i></i><button>Hide Solution</button></span>

# In[ ]:


get_ipython().run_cell_magic('writefile', 'inference_model_job.sh', '#!/bin/bash\n\nexec 1>/output/stdout.log 2>/output/stderr.log\n\nmkdir -p /output\n\nDEVICE=$1\nMODELPATH=$2\nSAVEPATH=$3\n\n\nsource /opt/intel/init_openvino.sh\naocl program acl0 /opt/intel/openvino/bitstreams/a10_vision_design_sg1_bitstreams/2019R4_PL1_FP16_MobileNet_Clamp.aocx\n\n\n# Run the load model python script\npython3 inference_on_device.py  --model_path ${MODELPATH} --device ${DEVICE} --path ${SAVEPATH}\n\ncd /output\n\ntar zcvf output.tgz *')


# ## Step 3a: Running on the FPGA and CPU
# 
# In the cell below, can you write the qsub command that will submit your job to the CPU?

# In[ ]:


fpga_cpu_job = # TODO: Write qsub command
print(fpga_cpu_job[0])


# <span class="graffiti-highlight graffiti-id_cvp3lyi-id_chmeh50"><i></i><button>Hide Solution</button></span>

# In[ ]:


fpga_cpu_job = get_ipython().getoutput('qsub inference_model_job.sh -d . -l nodes=1:tank-870:i5-6500te:iei-mustang-f100-a10 -F "HETERO:FPGA,CPU /data/models/intel/vehicle-license-plate-detection-barrier-0106/FP16/vehicle-license-plate-detection-barrier-0106 fpga_cpu_stats" -N store_core ')
print(fpga_cpu_job[0])


# ## Step 3b: Running on GPU and CPU

# In[ ]:


gpu_cpu_job = # TODO: Write qsub command
print(gpu_cpu_job[0])


# <span class="graffiti-highlight graffiti-id_7k34s6u-id_022l4bj"><i></i><button>Hide Solution</button></span>

# In[140]:


gpu_cpu_job = get_ipython().getoutput('qsub inference_model_job.sh -d . -l nodes=tank-870:i5-6500te:intel-hd-530 -F "HETERO:GPU,CPU /data/models/intel/vehicle-license-plate-detection-barrier-0106/FP16/vehicle-license-plate-detection-barrier-0106 gpu_cpu_stats" -N store_core ')
print(gpu_cpu_job[0])


# ## Step 3c: Running on FPGA, GPU and CPU

# In[ ]:


fpga_gpu_cpu_job = # TODO: Write qsub command
print(fpga_gpu_cpu_job[0])


# <span class="graffiti-highlight graffiti-id_mxh5ozv-id_qicoukm"><i></i><button>Hide Solution</button></span>

# In[154]:


fpga_gpu_cpu_job = get_ipython().getoutput('qsub inference_model_job.sh -d . -l nodes=tank-870:i5-6500te:intel-hd-530:iei-mustang-f100-a10 -F "HETERO:FPGA,GPU,CPU /data/models/intel/vehicle-license-plate-detection-barrier-0106/FP16/vehicle-license-plate-detection-barrier-0106 fpga_gpu_cpu_stats" -N store_core ')
print(fpga_gpu_cpu_job[0])


# ## Step 4: Getting the Live Stat Values
# 
# By running the below command, we can see the live status of the commands.

# <span class="graffiti-highlight graffiti-id_clj7fxa-id_d3gqjz0"><i></i><button>Graffiti Sample Button (edit me)</button></span>

# In[ ]:


import liveQStat
liveQStat.liveQStat()


# ## Step 5a: Get the results for FPGA and CPU
# 
# Running the cell below will get the output files from our job

# <span class="graffiti-highlight graffiti-id_cygruth-id_6nd1x96"><i></i><button>Graffiti Sample Button (edit me)</button></span>

# In[168]:


import get_results

get_results.getResults(fpga_cpu_job[0], get_stderr=True, filename="output.tgz", blocking=True)


# In[169]:


get_ipython().system('tar zxf output.tgz')


# In[170]:


get_ipython().system('cat stdout.log')


# ## Step 5b: Get the result for GPU and CPU

# In[171]:


import get_results

get_results.getResults(gpu_cpu_job[0], filename="output.tgz", blocking=True)


# In[172]:


get_ipython().system('tar zxf output.tgz')


# In[173]:


get_ipython().system('cat stdout.log')


# ## Step 5c: Get the result for FPGA, GPU and CPU

# In[174]:


import get_results

get_results.getResults(fpga_gpu_cpu_job[0], filename="output.tgz", blocking=True)


# In[175]:


get_ipython().system('tar zxf output.tgz')


# In[176]:


get_ipython().system('cat stdout.log')


# ## Step 6: View the Outputs
# 
# Can you plot the load time, inference time and the frames per second in the cell below?

# In[ ]:


import matplotlib.pyplot as plt

#File Paths to stats files
paths=[]

# TODO: Plot the different stats


# <span class="graffiti-highlight graffiti-id_m9kxw9k-id_4h5tl2h"><i></i><button>Hide Solution</button></span>

# In[ ]:


import matplotlib.pyplot as plt

def plot(labels, data, title, label):
    fig = plt.figure()
    ax = fig.add_axes([0,0,1,1])
    ax.set_ylabel(label)
    ax.set_title(title)
    ax.bar(labels, data)
    
def read_files(paths, labels):
    load_time=[]
    inference_time=[]
    fps=[]
    
    for path in paths:
        if os.path.isfile(path):
            f=open(path, 'r')
            load_time.append(float(f.readline()))
            inference_time.append(float(f.readline()))
            fps.append(float(f.readline()))

    plot(labels, load_time, 'Model Load Time', 'seconds')
    plot(labels, inference_time, 'Inference Time', 'seconds')
    plot(labels, fps, 'Frames per Second', 'Frames')

paths=['fpga_cpu_stats.txt', 'gpu_cpu_stats.txt', 'fpga_gpu_cpu_stats.txt']
read_files(paths, ['FPGA/CPU', 'GPU/CPU', 'FPGA/GPU/CPU'])


# In[ ]:




