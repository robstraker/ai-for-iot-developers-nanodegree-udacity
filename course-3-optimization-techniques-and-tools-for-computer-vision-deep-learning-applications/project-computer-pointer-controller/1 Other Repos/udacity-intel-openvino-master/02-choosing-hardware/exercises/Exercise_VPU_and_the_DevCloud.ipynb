{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: VPU and the DevCloud\n",
    "\n",
    "In this exercise, we will load a model on to the VPU, the neural compute stick in particular, and perform an inference on it. We will then calculate the time it takes to do the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Set up paths so we can run Dev Cloud utilities\n",
    "You *must* run this every time they enter a Workspace session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/intel_devcloud_support\n"
     ]
    }
   ],
   "source": [
    "%env PATH=/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/intel_devcloud_support\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('/opt/intel_devcloud_support'))\n",
    "sys.path.insert(0, os.path.abspath('/opt/intel'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "We will be using the `vehicle-license-plate-detection-barrier-0106` model for this exercise. Remember that to run a model on the VPU, we need to use FP16 as the model precision.\n",
    "\n",
    "The model is present in the `/data/models/intel` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Creating a Python Script\n",
    "\n",
    "The first step is to create a python script that you can use to load the model and perform an inference. I have used the `writefile` magic to create a python file called `inference.py`. You will need to complete this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference.py\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "from openvino.inference_engine import IENetwork\n",
    "from openvino.inference_engine import IECore\n",
    "import argparse\n",
    "\n",
    "def main(args):\n",
    "    model=args.model_path\n",
    "    model_weights=model+'.bin'\n",
    "    model_structure=model+'.xml'\n",
    "    \n",
    "    start=time.time()\n",
    "    \n",
    "    # TODO: Load the model on VPU\n",
    "    print(\"Starting...\")\n",
    "    print(\"Device:\", args.device)\n",
    "    print(\"Model:\", args.model_path)\n",
    "\n",
    "    core = IECore()\n",
    "    print(\"Core created\")\n",
    "    \n",
    "    network = IENetwork(model_structure, model_weights)\n",
    "    print(\"Network created\")\n",
    "    \n",
    "    exec_network = core.load_network(network, args.device, num_requests=1)\n",
    "    print(\"Network loaded\")\n",
    "    \n",
    "    print(f\"Time taken to load model = {time.time()-start} seconds\")\n",
    "    \n",
    "    # Reading and Preprocessing Image\n",
    "    input_img=cv2.imread('car.png')\n",
    "    input_img=cv2.resize(input_img, (300,300), interpolation = cv2.INTER_AREA)\n",
    "    input_img=np.moveaxis(input_img, -1, 0)\n",
    "\n",
    "    # TODO: Prepare the model for inference (create input dict etc.)\n",
    "    input_key = next(iter(network.inputs))\n",
    "    input_dict = {input_key:input_img}\n",
    "    print(\"Input created\")\n",
    "    \n",
    "    start=time.time()\n",
    "    for i in range(10):\n",
    "        # TODO: Run Inference in a Loop\n",
    "        exec_network.infer(input_dict)\n",
    "        print(f\"{i} Inference completed\")\n",
    "    \n",
    "    print(f\"Time Taken to run 10 Infernce on GPU is = {time.time()-start} seconds\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser=argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_path', required=True)\n",
    "    parser.add_argument('--device', default=None)\n",
    "    \n",
    "    args=parser.parse_args() \n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Creating a job submission script\n",
    "\n",
    "To submit a job to the devcloud, we need to create a script. I have named the script as `inference_job.sh`.\n",
    "\n",
    "Can you write a script that will take the model path and device as a command line argument and then call the python file you created in the previous cell with the path to the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference_job.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference_job.sh\n",
    "\n",
    "#TODO: Create job submission script\n",
    "exec 1>/output/stdout.log 2>/output/stderr.log\n",
    "mkdir -p /output\n",
    "\n",
    "DEVICE=$1\n",
    "MODELPATH=$2\n",
    "\n",
    "# Run the load model python script\\n\",\n",
    "python3 inference.py  --model_path ${MODELPATH} --device ${DEVICE}\n",
    "\n",
    "cd /output\n",
    "tar zcvf output.tgz stdout.log stderr.log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Running on the Device\n",
    "\n",
    "In the cell below, can you write the qsub command that will submit your job to the CPU, GPU or VPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TwUpDFff4y7WgekghqK10jqwjnDbffK1\n"
     ]
    }
   ],
   "source": [
    "DEVICE_CPU = 'CPU'\n",
    "DEVICE_GPU = 'GPU'\n",
    "DEVICE_VPU = 'MYRIAD'\n",
    "MODEL_FP32 = '/data/models/intel/vehicle-license-plate-detection-barrier-0106/FP32/vehicle-license-plate-detection-barrier-0106'\n",
    "MODEL_FP16 = '/data/models/intel/vehicle-license-plate-detection-barrier-0106/FP16/vehicle-license-plate-detection-barrier-0106'\n",
    "NODE_CPU = 'tank-870:e3-1268l-v5'\n",
    "NODE_GPU = 's002-n011:i5-6500te:intel-hd-530'\n",
    "NODE_VPU = 'tank-870:i5-6500te:intel-ncs2'\n",
    "\n",
    "#job_id = !qsub inference_job.sh -d . -l nodes=1:{NODE_CPU} -F \"{DEVICE_CPU} {MODEL_FP32}\" -N store_core\n",
    "#job_id = !qsub inference_job.sh -d . -l nodes=1:{NODE_GPU} -F \"{DEVICE_GPU} {MODEL_FP16}\" -N store_core\n",
    "job_id = !qsub inference_job.sh -d . -l nodes=1:{NODE_VPU} -F \"{DEVICE_VPU} {MODEL_FP16}\" -N store_core\n",
    "\n",
    "print(job_id[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Getting the Live Stat Values\n",
    "\n",
    "By running the below command, we can see the live status of the commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import liveQStat\n",
    "liveQStat.liveQStat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Get the results\n",
    "\n",
    "Running the cell below will get the output files from our job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getResults() is blocking until results of the job (id:TwUpDFff4y7WgekghqK10jqwjnDbffK1) are ready.\n",
      "Please wait........................................................Success!\n",
      "output.tgz was downloaded in the same folder as this notebook.\n"
     ]
    }
   ],
   "source": [
    "import get_results\n",
    "\n",
    "get_results.getResults(job_id[0], filename=\"output.tgz\", blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting...\r\n",
      "Device: MYRIAD\r\n",
      "Model: /data/models/intel/vehicle-license-plate-detection-barrier-0106/FP32/vehicle-license-plate-detection-barrier-0106\r\n",
      "Core created\r\n",
      "Network created\r\n",
      "\u001b[35mE: [ncAPI] [    824997] [python3] ncDeviceOpen:1012\tFailed to find booted device after boot\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!tar zxf output.tgz\n",
    "!cat stdout.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"inference.py\", line 56, in <module>\r\n",
      "    main(args)\r\n",
      "  File \"inference.py\", line 27, in main\r\n",
      "    exec_network = core.load_network(network, args.device, num_requests=1)\r\n",
      "  File \"ie_api.pyx\", line 134, in openvino.inference_engine.ie_api.IECore.load_network\r\n",
      "  File \"ie_api.pyx\", line 141, in openvino.inference_engine.ie_api.IECore.load_network\r\n",
      "RuntimeError: Can not init Myriad device: NC_ERROR\r\n",
      "tar: stdout.log: file changed as we read it\r\n"
     ]
    }
   ],
   "source": [
    "!cat stderr.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
