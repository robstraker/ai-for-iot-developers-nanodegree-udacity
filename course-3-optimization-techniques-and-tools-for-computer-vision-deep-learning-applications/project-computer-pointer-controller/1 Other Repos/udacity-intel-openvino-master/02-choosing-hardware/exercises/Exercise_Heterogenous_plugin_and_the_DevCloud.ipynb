{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_qwukc5b"
   },
   "source": [
    "# Exercise: Heterogenous Plugin and the DevCloud\n",
    "\n",
    "In this exercise, we will load a model using the hetero plugin on to the FPGA and CPU, and the GPU and CPU. We will then perform an inference on it and compare the time it takes to do the same for each device pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_z8bfs11"
   },
   "source": [
    "<span class=\"graffiti-highlight graffiti-id_z8bfs11-id_d97ox8f\"><i></i><button>Graffiti Sample Button (edit me)</button></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_0untint"
   },
   "source": [
    "\n",
    "\n",
    "#### Set up paths so we can run Dev Cloud utilities\n",
    "You *must* run this every time they enter a Workspace session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "graffitiCellId": "id_axn1sb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/intel_devcloud_support\n"
     ]
    }
   ],
   "source": [
    "%env PATH=/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/intel_devcloud_support\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('/opt/intel_devcloud_support'))\n",
    "sys.path.insert(0, os.path.abspath('/opt/intel'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_mhiayyz"
   },
   "source": [
    "## The model\n",
    "\n",
    "We will be using the `vehicle-license-plate-detection-barrier-0106` model for this exercise. Remember that to run a model using the HETERO Plugin, we need to use FP16 as the model precision.\n",
    "\n",
    "The model is present in the `/data/models/intel` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_ltf95ei"
   },
   "source": [
    "# Step 1: Creating a Python Script\n",
    "\n",
    "The first step is to create a python script that you can use to load the model and perform an inference. I have used the `writefile` magic to create a python file called `inference_on_device.py`. You will need to complete this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "graffitiCellId": "id_bpywo8s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference_on_device.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference_on_device.py\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "from openvino.inference_engine import IENetwork\n",
    "from openvino.inference_engine import IECore\n",
    "import argparse\n",
    "\n",
    "def main(args):\n",
    "    model=args.model_path\n",
    "    model_weights=model+'.bin'\n",
    "    model_structure=model+'.xml'\n",
    "    \n",
    "    start=time.time()\n",
    "    print('Started')\n",
    "    \n",
    "    # TODO: Load the model on VPU\n",
    "    model=IENetwork(model_structure, model_weights)\n",
    "    print('Model loaded')\n",
    "\n",
    "    core = IECore()\n",
    "    print('Core created')\n",
    "    \n",
    "    net = core.load_network(network=model, device_name=args.device, num_requests=1)\n",
    "    print('Network loaded')\n",
    "\n",
    "    print(f\"Time taken to load model = {time.time()-start} seconds\")\n",
    "    \n",
    "    # Reading and Preprocessing Image\n",
    "    input_img=cv2.imread('/data/resources/car.png')\n",
    "    input_img=cv2.resize(input_img, (300,300), interpolation = cv2.INTER_AREA)\n",
    "    input_img=np.moveaxis(input_img, -1, 0)\n",
    "    print('Image prepared')\n",
    "\n",
    "    # TODO: Prepare the model for inference (create input dict etc.)\n",
    "    input_name=next(iter(model.inputs))\n",
    "    input_dict={input_name:input_img}\n",
    "    print('Input created')\n",
    "    \n",
    "    start=time.time()\n",
    "    for _ in range(10):\n",
    "        # TODO: Run Inference in a Loop\n",
    "        net.infer(input_dict)\n",
    "        print('Inference performed')\n",
    "    \n",
    "    print(f\"Time Taken to run 10 Infernce using HETERO plugin is = {time.time()-start} seconds\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser=argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_path', required=True)\n",
    "    parser.add_argument('--device', default=None)\n",
    "    \n",
    "    args=parser.parse_args() \n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_1rnmf5g"
   },
   "source": [
    "<span class=\"graffiti-highlight graffiti-id_1rnmf5g-id_nmeqj1a\"><i></i><button>Show Solution</button></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_ufbi2ll"
   },
   "source": [
    "## Step 2: Creating a job submission script\n",
    "\n",
    "To submit a job to the devcloud, we need to create a script. I have named the script as `inference_hetero_model_job.sh`.\n",
    "\n",
    "Can you write a script that will take the model path and device as a command line argument and then call the python file you created in the previous cell with the path to the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "graffitiCellId": "id_5r13clu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference_fpga_model_job.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference_fpga_model_job.sh\n",
    "#!/bin/bash\n",
    "\n",
    "exec 1>/output/stdout.log 2>/output/stderr.log\n",
    "\n",
    "mkdir -p /output\n",
    "\n",
    "DEVICE=$1\n",
    "MODELPATH=$2\n",
    "\n",
    "\n",
    "source /opt/intel/init_openvino.sh\n",
    "aocl program acl0 /opt/intel/openvino/bitstreams/a10_vision_design_sg1_bitstreams/2019R4_PL1_FP16_MobileNet_Clamp.aocx\n",
    "\n",
    "\n",
    "# Run the load model python script\n",
    "python3 inference_on_device.py  --model_path ${MODELPATH} --device ${DEVICE}\n",
    "\n",
    "cd /output\n",
    "\n",
    "tar zcvf output.tgz stdout.log stderr.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_f1nbmn9"
   },
   "source": [
    "<span class=\"graffiti-highlight graffiti-id_f1nbmn9-id_ia7yjlq\"><i></i><button>Show Solution</button></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_28fed2h"
   },
   "source": [
    "## Step 3a: Running on the FPGA\n",
    "\n",
    "In the cell below, can you write the qsub command that will submit your job to the CPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "graffitiCellId": "id_6awpacu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B8MXnXsIQ0PAFpRaO4LaRcI9Knp3nXTq\n"
     ]
    }
   ],
   "source": [
    "fpga_job = !qsub inference_fpga_model_job.sh -d . -l nodes=1:tank-870:i5-6500te:iei-mustang-f100-a10 -F \"HETERO:FPGA,CPU /data/models/intel/vehicle-license-plate-detection-barrier-0106/FP32/vehicle-license-plate-detection-barrier-0106\" -N store_core \n",
    "print(fpga_job[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_cvp3lyi"
   },
   "source": [
    "<span class=\"graffiti-highlight graffiti-id_cvp3lyi-id_chmeh50\"><i></i><button>Show Solution</button></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_io25c53"
   },
   "source": [
    "## Step 3b: Running on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "graffitiCellId": "id_v5klpi1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E8Au22grFc6XKlo7Pm4U7wxsltq3veVh\n"
     ]
    }
   ],
   "source": [
    "gpu_job = !qsub inference_fpga_model_job.sh -d . -l nodes=s002-n011:i5-6500te:intel-hd-530 -F \"HETERO:GPU,CPU /data/models/intel/vehicle-license-plate-detection-barrier-0106/FP32/vehicle-license-plate-detection-barrier-0106\" -N store_core \n",
    "print(gpu_job[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_7k34s6u"
   },
   "source": [
    "<span class=\"graffiti-highlight graffiti-id_7k34s6u-id_022l4bj\"><i></i><button>Show Solution</button></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_8ym2smn"
   },
   "source": [
    "## Step 4: Getting the Live Stat Values\n",
    "\n",
    "By running the below command, we can see the live status of the commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_clj7fxa"
   },
   "source": [
    "<span class=\"graffiti-highlight graffiti-id_clj7fxa-id_d3gqjz0\"><i></i><button>Graffiti Sample Button (edit me)</button></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "graffitiCellId": "id_zig7qg5"
   },
   "outputs": [],
   "source": [
    "import liveQStat\n",
    "liveQStat.liveQStat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_2vp5y4m"
   },
   "source": [
    "## Step 5a: Get the results for FPGA\n",
    "\n",
    "Running the cell below will get the output files from our job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_cygruth"
   },
   "source": [
    "<span class=\"graffiti-highlight graffiti-id_cygruth-id_6nd1x96\"><i></i><button>Graffiti Sample Button (edit me)</button></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "graffitiCellId": "id_zpdshwo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getResults() is blocking until results of the job (id:B8MXnXsIQ0PAFpRaO4LaRcI9Knp3nXTq) are ready.\n",
      "Please wait..............Success!\n",
      "output.tgz was downloaded in the same folder as this notebook.\n"
     ]
    }
   ],
   "source": [
    "import get_results\n",
    "\n",
    "get_results.getResults(fpga_job[0], filename=\"output.tgz\", blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "graffitiCellId": "id_0quk13q"
   },
   "outputs": [],
   "source": [
    "!tar zxf output.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "graffitiCellId": "id_l1gs5j5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\r\n",
      "Model loaded\r\n",
      "Core created\r\n"
     ]
    }
   ],
   "source": [
    "!cat stdout.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "graffitiCellId": "id_5ll6r4x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./submission.sh: line 11: /opt/intel/init_openvino.sh: No such file or directory\r\n",
      "./submission.sh: line 12: aocl: command not found\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"inference_on_device.py\", line 51, in <module>\r\n",
      "    main(args)\r\n",
      "  File \"inference_on_device.py\", line 24, in main\r\n",
      "    net = core.load_network(network=model, device_name=args.device, num_requests=1)\r\n",
      "  File \"ie_api.pyx\", line 134, in openvino.inference_engine.ie_api.IECore.load_network\r\n",
      "  File \"ie_api.pyx\", line 141, in openvino.inference_engine.ie_api.IECore.load_network\r\n",
      "RuntimeError: Failed to create plugin libdliaPlugin.so for device FPGA\r\n",
      "Please, check your environment\r\n",
      "Cannot load library 'libdliaPlugin.so': libdliaPlugin.so: cannot open shared object file: No such file or directory\r\n",
      "\r\n",
      "tar: stdout.log: file changed as we read it\r\n"
     ]
    }
   ],
   "source": [
    "!cat stderr.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_gykmtow"
   },
   "source": [
    "## Step 5b: Get the result for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "graffitiCellId": "id_8w79u8w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getResults() is blocking until results of the job (id:E8Au22grFc6XKlo7Pm4U7wxsltq3veVh) are ready.\n",
      "Please wait..........................................Success!\n",
      "output.tgz was downloaded in the same folder as this notebook.\n"
     ]
    }
   ],
   "source": [
    "import get_results\n",
    "\n",
    "get_results.getResults(gpu_job[0], filename=\"output.tgz\", blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "graffitiCellId": "id_kv4qcd6"
   },
   "outputs": [],
   "source": [
    "!tar zxf output.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "graffitiCellId": "id_etgr4le"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DetectionOutput_Reshape_priors_/Output_0/Data__const is CPU\r\n",
      "DetectionOutput_Reshape_conf_ is CPU\r\n",
      "SSD/concat_reshape_softmax/mbox_conf_final is CPU\r\n",
      "SSD/concat_reshape_softmax/Reshape is GPU\r\n",
      "SSD/concat_reshape_softmax/mbox_conf_logits is GPU\r\n",
      "SSD/ssd_head_1/Flatten_1/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head_1/layer_18/output_mbox_conf/BiasAdd/Add/Transpose is GPU\r\n",
      "DetectionOutput_Reshape_loc_ is CPU\r\n",
      "SSD/concat_reshape_softmax/mbox_loc_final is CPU\r\n",
      "SSD/ssd_head_1/Flatten/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head_1/layer_18/output_mbox_loc/BiasAdd/Add/Transpose is GPU\r\n",
      "SSD/ssd_head_2/Flatten_1/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head_2/feature_map_1_mbox_conf/BiasAdd/Add/Transpose is GPU\r\n",
      "SSD/ssd_head_2/Flatten/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head_2/feature_map_1_mbox_loc/BiasAdd/Add/Transpose is GPU\r\n",
      "SSD/ssd_head_3/Flatten_1/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head_3/feature_map_2_mbox_conf/BiasAdd/Add/Transpose is GPU\r\n",
      "SSD/ssd_head_3/Flatten/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head_3/feature_map_2_mbox_loc/BiasAdd/Add/Transpose is GPU\r\n",
      "SSD/ssd_head_4/Flatten_1/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head_4/feature_map_3_mbox_conf/BiasAdd/Add/Transpose is GPU\r\n",
      "SSD/ssd_head_4/Flatten/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head_4/feature_map_3_mbox_loc/BiasAdd/Add/Transpose is GPU\r\n",
      "SSD/ssd_head_5/Flatten_1/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head_5/feature_map_4_mbox_conf/BiasAdd/Add/Transpose is GPU\r\n",
      "SSD/ssd_head_5/Flatten/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head_5/feature_map_4_mbox_loc/BiasAdd/Add/Transpose is GPU\r\n",
      "SSD/ssd_head/Flatten_1/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head/layer_14/output_mbox_conf/BiasAdd/Add/Transpose is GPU\r\n",
      "SSD/ssd_head/Flatten/flatten/Reshape is GPU\r\n",
      "SSD/ssd_head/layer_14/output_mbox_loc/BiasAdd/Add/Transpose is GPU\r\n",
      "Started\r\n",
      "Model loaded\r\n",
      "Core created\r\n",
      "Network loaded\r\n",
      "Time taken to load model = 26.048017978668213 seconds\r\n",
      "Image prepared\r\n",
      "Input created\r\n",
      "Inference performed\r\n",
      "Inference performed\r\n",
      "Inference performed\r\n",
      "Inference performed\r\n",
      "Inference performed\r\n",
      "Inference performed\r\n",
      "Inference performed\r\n",
      "Inference performed\r\n",
      "Inference performed\r\n",
      "Inference performed\r\n",
      "Time Taken to run 10 Infernce using HETERO plugin is = 0.07474851608276367 seconds\r\n"
     ]
    }
   ],
   "source": [
    "!cat stdout.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "graffitiCellId": "id_4rf323l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./submission.sh: line 11: /opt/intel/init_openvino.sh: No such file or directory\r\n",
      "./submission.sh: line 12: aocl: command not found\r\n",
      "tar: stdout.log: file changed as we read it\r\n"
     ]
    }
   ],
   "source": [
    "!cat stderr.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "graffitiCellId": "id_dmut4ng"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "graffiti": {
   "firstAuthorId": "dca260a8-2142-11ea-b0f7-6f7abbbf2f85",
   "id": "id_610hfgn",
   "language": "EN"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
