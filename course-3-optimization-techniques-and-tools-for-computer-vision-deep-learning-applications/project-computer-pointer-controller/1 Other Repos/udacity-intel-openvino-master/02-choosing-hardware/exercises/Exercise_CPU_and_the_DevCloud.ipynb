{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_bffwl15"
   },
   "source": [
    "# Exercise: CPU and the DevCloud\n",
    "\n",
    "In this exercise, we will load a model on to the Intel Xeon CPU and perform an inference on it. We will then calculate the time it takes to do the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_g9b3e7l"
   },
   "source": [
    "<span class=\"graffiti-highlight graffiti-id_g9b3e7l-id_08m53df\"><i></i><button>Graffiti Sample Button (edit me)</button></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_oh4y2b6"
   },
   "source": [
    "\n",
    "\n",
    "#### Set up paths so we can run Dev Cloud utilities\n",
    "You *must* run this every time they enter a Workspace session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "graffitiCellId": "id_gdynvhk"
   },
   "outputs": [],
   "source": [
    "%env PATH=/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/intel_devcloud_support\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('/opt/intel_devcloud_support'))\n",
    "sys.path.insert(0, os.path.abspath('/opt/intel'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_bjf1j90"
   },
   "source": [
    "## The model\n",
    "\n",
    "We will be using the `vehicle-license-plate-detection-barrier-0106` model for this exercise. Remember that to run a model on the CPU, we need to use FP32 as the model precision.\n",
    "\n",
    "The model is present in the `/data/models/intel` folder.\n",
    "\n",
    "We will be running inference on an image of a car. The path to the image is `/data/resources/car.png`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_ggujukc"
   },
   "source": [
    "# Step 1: Creating a Python Script\n",
    "\n",
    "The first step is to create a python script that you can use to load the model and perform an inference. I have used the `writefile` magic to create a python file called `inference_cpu_model.py`. You will need to complete this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "graffitiCellId": "id_zi56gxx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference_cpu_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference_cpu_model.py\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "from openvino.inference_engine import IENetwork\n",
    "from openvino.inference_engine import IECore\n",
    "from openvino.inference_engine import IECore\n",
    "import argparse\n",
    "\n",
    "def main(args):\n",
    "    model=args.model_path\n",
    "    model_weights=model+'.bin'\n",
    "    model_structure=model+'.xml'\n",
    "    \n",
    "    start=time.time()\n",
    "    \n",
    "    print(\"Starting... Model:\", model)\n",
    "    \n",
    "    # TODO: Load the model\n",
    "    plugin = IECore()\n",
    "    print(\"Inference Engine plugin created\")\n",
    "\n",
    "    network = IENetwork(model_structure, model_weights)\n",
    "    print(\"Network created from spec\")\n",
    "\n",
    "    exec_network = plugin.load_network(network, 'CPU', num_requests=1)\n",
    "    print(\"Network loaded into Inference Engine\")\n",
    "    \n",
    "    print(f\"Time taken to load model = {time.time()-start} seconds\")\n",
    "    \n",
    "    # Reading and Preprocessing Image\n",
    "    input_img=cv2.imread('/data/resources/car.png')\n",
    "    input_img=cv2.resize(input_img, (300,300), interpolation = cv2.INTER_AREA)\n",
    "    input_img=np.moveaxis(input_img, -1, 0)\n",
    "    print(\"Input image prepared\")\n",
    "\n",
    "    # TODO: Prepare the model for inference (create input dict etc.)\n",
    "    input_key = next(iter(network.inputs))\n",
    "    input_dict = {input_key:input_img}\n",
    "    print(\"Input prepared, key:\", input_key)\n",
    "    \n",
    "    start=time.time()\n",
    "    for i in range(10):\n",
    "        # TODO: Run Inference in a Loop\n",
    "        print(\"Inference #\", i, \"start...\")\n",
    "        exec_network.infer(input_dict)\n",
    "        print(\"Inference #\", i, \"finished\")\n",
    "    \n",
    "    print(f\"Time Taken to run 10 Infernce on CPU is = {time.time()-start} seconds\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser=argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_path', required=True)\n",
    "    \n",
    "    args=parser.parse_args() \n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_6t269sv"
   },
   "source": [
    "<span class=\"graffiti-highlight graffiti-id_6t269sv-id_2g8nwk3\"><i></i><button>Show Solution</button></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_cc87wy7"
   },
   "source": [
    "## Step 2: Creating a job submission script\n",
    "\n",
    "To submit a job to the devcloud, we need to create a script. I have named the script as `inference_cpu_model_job.sh`.\n",
    "\n",
    "Can you write a script that will take the model path as a command line argument and then call the python file you created in the previous cell with the path to the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "graffitiCellId": "id_63f8u60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference_cpu_model_job.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference_cpu_model_job.sh\n",
    "\n",
    "exec 1>/output/stdout.log 2>/output/stderr.log\n",
    "mkdir -p /output\n",
    "\n",
    "#TODO: Create job submission script\n",
    "MODELPATH=$1\n",
    "\n",
    "# Run the load model python script\n",
    "python3 inference_cpu_model.py --model_path ${MODELPATH}\n",
    "\n",
    "cd /output\n",
    "tar zcvf output.tgz stdout.log stderr.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_vc779df"
   },
   "source": [
    "<span class=\"graffiti-highlight graffiti-id_vc779df-id_z9ijl86\"><i></i><button>Show Solution</button></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_oo8pxmq"
   },
   "source": [
    "## Step 3: Running on the CPU\n",
    "\n",
    "In the cell below, can you write the qsub command that will submit your job to the Intel CPU?\n",
    "\n",
    "To get a list of hardware devices available on the DevCloud, you can go to [this link](https://devcloud.intel.com/edge/get_started/devcloud/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "graffitiCellId": "id_7d8382j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bYEM1CkVlr3eSmw23zEH4TCxAPZfdrlw\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write qsub command\n",
    "job_id_core = !qsub inference_cpu_model_job.sh -d . -l nodes=1:tank-870:e3-1268l-v5 -F \"/data/models/intel/vehicle-license-plate-detection-barrier-0106/FP32/vehicle-license-plate-detection-barrier-0106\" -N store_core \n",
    "print(job_id_core[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_qnrfru0"
   },
   "source": [
    "<span class=\"graffiti-highlight graffiti-id_qnrfru0-id_7ofr7nk\"><i></i><button>Show Solution</button></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_w5391uo"
   },
   "source": [
    "## Step 4: Getting the Live Stat Values\n",
    "\n",
    "By running the below command, we can see the live status of the commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_9xnofi7"
   },
   "source": [
    "<span class=\"graffiti-highlight graffiti-id_9xnofi7-id_m9v5xi8\"><i></i><button>Graffiti Sample Button (edit me)</button></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "graffitiCellId": "id_tajznvh"
   },
   "outputs": [],
   "source": [
    "import liveQStat\n",
    "liveQStat.liveQStat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_c30co52"
   },
   "source": [
    "## Step 5: Get the results\n",
    "\n",
    "Running the cell below will get the output files from our job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "graffitiCellId": "id_nnbbmjj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getResults() is blocking until results of the job (id:bYEM1CkVlr3eSmw23zEH4TCxAPZfdrlw) are ready.\n",
      "Please wait............................................................................Success!\n",
      "output.tgz was downloaded in the same folder as this notebook.\n"
     ]
    }
   ],
   "source": [
    "import get_results\n",
    "\n",
    "get_results.getResults(job_id_core[0], get_stderr=True, filename=\"output.tgz\", blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "graffitiCellId": "id_s4csu1d"
   },
   "outputs": [],
   "source": [
    "!tar zxf output.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "graffitiCellId": "id_tp3wz74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting... Model: /data/models/intel/vehicle-license-plate-detection-barrier-0106/FP32/vehicle-license-plate-detection-barrier-0106\r\n",
      "Inference Engine plugin created\r\n",
      "Network created from spec\r\n",
      "Network loaded into Inference Engine\r\n",
      "Time taken to load model = 0.9887981414794922 seconds\r\n",
      "Input image prepared\r\n",
      "Input prepared, key: Placeholder\r\n",
      "Inference # 0 start...\r\n",
      "Inference # 0 finished\r\n",
      "Inference # 1 start...\r\n",
      "Inference # 1 finished\r\n",
      "Inference # 2 start...\r\n",
      "Inference # 2 finished\r\n",
      "Inference # 3 start...\r\n",
      "Inference # 3 finished\r\n",
      "Inference # 4 start...\r\n",
      "Inference # 4 finished\r\n",
      "Inference # 5 start...\r\n",
      "Inference # 5 finished\r\n",
      "Inference # 6 start...\r\n",
      "Inference # 6 finished\r\n",
      "Inference # 7 start...\r\n",
      "Inference # 7 finished\r\n",
      "Inference # 8 start...\r\n",
      "Inference # 8 finished\r\n",
      "Inference # 9 start...\r\n",
      "Inference # 9 finished\r\n",
      "Time Taken to run 10 Infernce on CPU is = 0.07056117057800293 seconds\r\n"
     ]
    }
   ],
   "source": [
    "!cat stdout.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "graffitiCellId": "id_5tx6j70"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "graffiti": {
   "firstAuthorId": "dca260a8-2142-11ea-b0f7-6f7abbbf2f85",
   "id": "id_eq2mpx3",
   "language": "EN"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
