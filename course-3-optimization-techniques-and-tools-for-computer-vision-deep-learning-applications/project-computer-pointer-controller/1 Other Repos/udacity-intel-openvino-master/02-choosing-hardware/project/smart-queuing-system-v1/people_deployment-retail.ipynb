{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart Queue Monitoring System - Retail Scenario\n",
    "\n",
    "In this project, you will build a people counter app to reduce congestion in queuing systems by guiding people to the least congested queue. You will have to use Intel's OpenVINO API and the person detection model from their open model zoo to build this project. It demonstrates how to create a smart video IoT solution using Intel® hardware and software tools. This solution detects people in a designated area, providing the number of people in the frame.\n",
    "\n",
    "## Overview of how it works\n",
    "Your code should read the equivalent of command line arguments and loads a network and image from the video input to the Inference Engine (IE) plugin. A job is submitted to an edge compute node with a hardware accelerator such as Intel® HD Graphics GPU, Intel® Movidius™ Neural Compute Stick 2 and Intel® Arria® 10 FPGA.\n",
    "After the inference is completed, the output videos are appropriately stored in the /results/[device] directory, which can then be viewed within the Jupyter Notebook instance.\n",
    "\n",
    "## Demonstration objectives\n",
    "* Video as input is supported using **OpenCV**\n",
    "* Inference performed on edge hardware (rather than on the development node hosting this Jupyter notebook)\n",
    "* **OpenCV** provides the bounding boxes, labels and other information\n",
    "* Visualization of the resulting bounding boxes\n",
    "\n",
    "\n",
    "## Step 0: Set Up\n",
    "\n",
    "### 0.1: Import dependencies\n",
    "\n",
    "Run the below cell to import Python dependencies needed for displaying the results in this notebook\n",
    "(tip: select the cell and use **Ctrl+enter** to run the cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import your dependencies here\n",
    "from demoTools.demoutils import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2  (Optional-step): Original video without inference\n",
    "\n",
    "If you are curious to see the input video, run the following cell to view the original video stream used for inference and people counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>People Counter Video</h2>\n",
       "    \n",
       "    <video alt=\"\" controls autoplay muted height=\"480\"><source src=\"./resources/retail.mp4\" type=\"video/mp4\" /></video>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videoHTML('People Counter Video', ['./resources/retail.mp4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Using Intel® Distribution of OpenVINO™ toolkit\n",
    "\n",
    "We will be using Intel® Distribution of OpenVINO™ toolkit Inference Engine (IE) to locate people in frame.\n",
    "There are five steps involved in this task:\n",
    "\n",
    "1. Download the model using the open_model_zoo\n",
    "2. Choose a device and create IEPlugin for the device\n",
    "3. Read the Model using IENetwork\n",
    "4. Load the IENetwork into the Plugin\n",
    "5. Run inference.\n",
    "\n",
    "### 1.1 Downloading Model\n",
    "\n",
    "Write a command to download the  **person-detection-retail-0013** model in an IR format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your command here\n",
    "\n",
    "# !wget https://download.01.org/opencv/2020/openvinotoolkit/2020.1/open_model_zoo/models_bin/1/person-detection-retail-0013/FP16/person-detection-retail-0013.xml\n",
    "# !wget https://download.01.org/opencv/2020/openvinotoolkit/2020.1/open_model_zoo/models_bin/1/person-detection-retail-0013/FP16/person-detection-retail-0013.bin    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1768\r\n",
      "drwx------ 2 u40443 u40443    4096 Apr  4 14:45 .\r\n",
      "drwxr-xr-x 9 u40443 u40443    4096 Apr 10 11:06 ..\r\n",
      "-rw------- 1 u40443 u40443 1445736 Jan 24 03:21 person-detection-retail-0013.bin\r\n",
      "-rw------- 1 u40443 u40443  354107 Jan 24 03:21 person-detection-retail-0013.xml\r\n"
     ]
    }
   ],
   "source": [
    "!ls -la model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Inference on a video\n",
    "\n",
    "By now you should have already completed the inference code in <a href=\"person_detect.py\">person_detect.py</a>. If you haven't done so already, then you should do it now.\n",
    "\n",
    "The Python code should take in command line arguments for video, model etc.\n",
    "\n",
    "While the type of command line options is up to you, the command below is an example \n",
    "\n",
    "```\n",
    "python3 main.py -m ${MODELPATH} \\\n",
    "                -i ${INPUT_FILE} \\\n",
    "                -o ${OUTPUT_FILE} \\\n",
    "                -d ${DEVICE} \\\n",
    "                -pt ${THRESHOLD}\\\n",
    "\n",
    "```\n",
    "\n",
    "##### The description of the arguments used in the argument parser is the command line executable equivalent.\n",
    "* -m location of the pre-trained IR model which has been pre-processed using the model optimizer. There is automated support built in this argument to support both FP32 and FP16 models targeting different hardware\n",
    "* -i  location of the input video stream\n",
    "* -o location where the output file with inference needs to be stored (results/[device])\n",
    "* -d type of Hardware Acceleration (CPU, GPU, MYRIAD, HDDL or HETERO:FPGA,CPU)\n",
    "* -pt probability threshold value for the person detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Creating job file\n",
    "\n",
    "To run inference on the video, we need more compute power.\n",
    "We will run the workload on several edge compute nodes present in the IoT DevCloud. We will send work to the edge compute nodes by submitting the corresponding non-interactive jobs into a queue. For each job, we will specify the type of the edge compute server that must be allocated for the job.\n",
    "\n",
    "The job file is written in Bash, and will be executed directly on the edge compute node.\n",
    "You will have to create the job file by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting person_detect_job.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile person_detect_job.sh\n",
    "# The writefile magic command can be used to create and save a file\n",
    "\n",
    "MODEL=$1\n",
    "DEVICE=$2\n",
    "VIDEO=$3\n",
    "QUEUE=$4\n",
    "OUTPUT=$5\n",
    "PEOPLE=$6\n",
    "THRESHOLD=$7\n",
    "\n",
    "mkdir -p $5\n",
    "\n",
    "if [ $DEVICE = \"HETERO:FPGA,CPU\" ]; then\n",
    "    #Environment variables and compilation for edge compute nodes with FPGAs\n",
    "    source /opt/intel/init_openvino.sh\n",
    "    aocl program acl0 /opt/intel/openvino/bitstreams/a10_vision_design_sg1_bitstreams/2019R4_PL1_FP16_MobileNet_Clamp.aocx\n",
    "fi\n",
    "\n",
    "echo \"Running person_detect.py\"\n",
    "echo \"Model: $MODEL\"\n",
    "echo \"Device: $DEVICE\"\n",
    "echo \"Video: $VIDEO\"\n",
    "echo \"Queue: $QUEUE\"\n",
    "echo \"Output: $OUTPUT\"\n",
    "echo \"People: $PEOPLE\"\n",
    "echo \"Threshold: $THRESHOLD\"\n",
    "\n",
    "python3 person_detect.py  --model ${MODEL} \\\n",
    "                          --visualise \\\n",
    "                          --queue_param ${QUEUE} \\\n",
    "                          --device ${DEVICE} \\\n",
    "                          --video ${VIDEO} \\\n",
    "                          --output_path ${OUTPUT} \\\n",
    "                          --max_people ${PEOPLE} \\\n",
    "                          --threshold ${THRESHOLD}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Understand how jobs are submitted into the queue\n",
    "\n",
    "Now that we have the job script, we can submit the jobs to edge compute nodes. In the IoT DevCloud, you can do this using the `qsub` command.\n",
    "We can submit people_counter to several different types of edge compute nodes simultaneously or just one node at a time.\n",
    "\n",
    "There are three options of `qsub` command that we use for this:\n",
    "- `-l` : this option let us select the number and the type of nodes using `nodes={node_count}:{property}`. \n",
    "- `-F` : this option let us send arguments to the bash script. \n",
    "- `-N` : this option let us name the job so that it is easier to distinguish between them.\n",
    "\n",
    "Example using `qsub` command:\n",
    "\n",
    "`!qsub person_detect_job.sh -l nodes=1:tank-870:i5-6500te -d . -F \"models/intel/PATH-TO-MODEL DEVICE resources/retail.mp4 bin/queue_param/retail.npy results/retail/DEVICE MAX-PEOPLE\" -N JOB-NAME`\n",
    "\n",
    "You will need to change the following variables, `models/intel/PATH-TO-MODEL`, `DEVICE`, `results/retail/DEVICE`, `MAX-PEOPLE`, and `JOB-NAME` to the appropriate values.\n",
    "\n",
    "If you are curious to see the available types of nodes on the IoT DevCloud, run the following optional cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pbsnodes | grep compnode | awk '{print $3}' | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the properties describe the node, and number on the left is the number of available nodes of that architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Job queue submission\n",
    "\n",
    "Each of the cells below should submit a job to different edge compute nodes.\n",
    "The output of the cell is the `JobID` of your job, which you can use to track progress of a job.\n",
    "\n",
    "**Note** You can submit all jobs at once or one at a time. \n",
    "\n",
    "After submission, they will go into a queue and run as soon as the requested compute resources become available. \n",
    "(tip: **shift+enter** will run the cell and automatically move you to the next cell. So you can hit **shift+enter** multiple times to quickly run multiple cells)\n",
    "\n",
    "If your job successfully runs and completes, it will output a video, `output_video.mp4`, and a text file, `stats.txt`, in the `results/retail/DEVICE` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants & helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'model/person-detection-retail-0013.xml'\n",
    "VIDEO = 'resources/retail.mp4'\n",
    "QUEUE = 'bin/queue_param/retail.npy'\n",
    "PEOPLE = 2\n",
    "THRESHOLD = 0.7\n",
    "\n",
    "\n",
    "def submit(device, node):\n",
    "    device_postfix = device\n",
    "    if device.startswith('HETERO:FPGA'):\n",
    "        device_postfix = 'FPGA'\n",
    "        \n",
    "    job_name = 'RETAIL_' + device_postfix\n",
    "    output = 'results/retail/' + device_postfix\n",
    "    params = '{} {} {} {} {} {} {}'.format(MODEL_PATH, device, VIDEO, QUEUE, output, PEOPLE, THRESHOLD)\n",
    "\n",
    "    job_id = !qsub person_detect_job.sh -l nodes=1:{node} -d . -F \"{params}\" -N {job_name}\n",
    "    job_id_number = job_id[0].split('.')[0]\n",
    "    \n",
    "    print(f'Job ID: {job_id}, #: {job_id_number}')\n",
    "    return job_id_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with an Intel® CPU\n",
    "In the cell below, write a script to submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank* 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel® Core™ i5-6500TE processor</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: ['32111.v-qsvr-1.devcloud-edge'], #: 32111\n"
     ]
    }
   ],
   "source": [
    "#Submit job to the queue\n",
    "job_number_cpu = submit('CPU', 'tank-870:i5-6500te')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel® Core CPU and using the onboard Intel® GPU\n",
    "In the cell below, write a script to submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank* 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel® Core i5-6500TE</a>. The inference workload will run on the Intel® HD Graphics 530 card integrated with the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: ['32112.v-qsvr-1.devcloud-edge'], #: 32112\n"
     ]
    }
   ],
   "source": [
    "#Submit job to the queue\n",
    "job_number_gpu = submit('GPU', 'tank-870:i5-6500te:intel-hd-530')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel® NCS 2 (Neural Compute Stick 2)\n",
    "In the cell below, write a script to submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core i5-6500te CPU</a>. The inference workload will run on an <a \n",
    "    href=\"https://software.intel.com/en-us/neural-compute-stick\">Intel Neural Compute Stick 2</a> installed in this  node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: ['32113.v-qsvr-1.devcloud-edge'], #: 32113\n"
     ]
    }
   ],
   "source": [
    "#Submit job to the queue\n",
    "job_number_ncs2 = submit('MYRIAD', 'tank-870:i5-6500te:intel-ncs2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with IEI Mustang-F100-A10 (Intel® Arria® 10 FPGA)\n",
    "In the cell below, write a script to submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core™ i5-6500te CPU</a> . The inference workload will run on the <a href=\"https://www.ieiworld.com/mustang-f100/en/\"> IEI Mustang-F100-A10 </a> card installed in this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: ['32114.v-qsvr-1.devcloud-edge'], #: 32114\n"
     ]
    }
   ],
   "source": [
    "#Submit job to the queue\n",
    "job_number_fpga = submit('HETERO:FPGA,CPU', 'tank-870:i5-6500te:iei-mustang-f100-a10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Check if the jobs are done\n",
    "\n",
    "To check on the jobs that were submitted, use a command to check the status of the job.\n",
    "\n",
    "Column `S` shows the state of your running jobs.\n",
    "\n",
    "For example:\n",
    "- If `JOB ID`is in Q state, it is in the queue waiting for available resources.\n",
    "- If `JOB ID` is in R state, it is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42bfc92e590f4ed5977365047ff3d4a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='1px solid gray', height='300px', width='100%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90cea866dfd41ae8a1ac32b4c565a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Stop', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enter your command here to check the status of your jobs\n",
    "%matplotlib inline\n",
    "liveQstat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Wait!***\n",
    "\n",
    "Please wait for the inference jobs and video rendering to complete before proceeding to the next step.\n",
    "\n",
    "## Step 3: View Results\n",
    "\n",
    "Write a short utility script that will display these videos within the notebook.\n",
    "\n",
    "*Tip*: See `demoutils.py` if you are interested in understanding further on how the results are displayed in notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "########################################################################\r\n",
      "#      Date:           Fri Apr 10 11:09:52 PDT 2020\r\n",
      "#    Job ID:           32111.v-qsvr-1.devcloud-edge\r\n",
      "#      User:           u40443\r\n",
      "# Resources:           neednodes=1:tank-870:i5-6500te,nodes=1:tank-870:i5-6500te,walltime=01:00:00\r\n",
      "########################################################################\r\n",
      "\r\n",
      "[setupvars.sh] OpenVINO environment initialized\r\n",
      "Running person_detect.py\r\n",
      "Model: model/person-detection-retail-0013.xml\r\n",
      "Device: CPU\r\n",
      "Video: resources/retail.mp4\r\n",
      "Queue: bin/queue_param/retail.npy\r\n",
      "Output: results/retail/CPU\r\n",
      "People: 2\r\n",
      "Threshold: 0.7\r\n",
      "Model loaded\r\n",
      "Core created\r\n",
      "Network loaded\r\n",
      "Input key: data input shape: [1, 3, 320, 544]\r\n",
      "Output key: detection_out\r\n",
      "Model loaded, loading time: 0:00:01.587233\r\n",
      "Total frames 167, processing time: 0:00:04.516149\r\n",
      "\r\n",
      "########################################################################\r\n",
      "# End of output for job 32111.v-qsvr-1.devcloud-edge\r\n",
      "# Date: Fri Apr 10 11:10:09 PDT 2020\r\n",
      "########################################################################\r\n",
      "\r\n",
      "skipping application metrics\r\n",
      "Your telemetry account is not created\r\n",
      "True\r\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>Results on CPU</h2>\n",
       "    \n",
       "    <video alt=\"\" controls autoplay muted height=\"480\"><source src=\"./results/retail/CPU/out.mp4\" type=\"video/mp4\" /></video>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write your script for Intel Core CPU video results here\n",
    "!cat RETAIL_CPU.o{job_number_cpu}\n",
    "videoHTML('Results on CPU', ['./results/retail/CPU/out.mp4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "########################################################################\r\n",
      "#      Date:           Fri Apr 10 11:09:56 PDT 2020\r\n",
      "#    Job ID:           32112.v-qsvr-1.devcloud-edge\r\n",
      "#      User:           u40443\r\n",
      "# Resources:           neednodes=1:tank-870:i5-6500te:intel-hd-530,nodes=1:tank-870:i5-6500te:intel-hd-530,walltime=01:00:00\r\n",
      "########################################################################\r\n",
      "\r\n",
      "[setupvars.sh] OpenVINO environment initialized\r\n",
      "Running person_detect.py\r\n",
      "Model: model/person-detection-retail-0013.xml\r\n",
      "Device: GPU\r\n",
      "Video: resources/retail.mp4\r\n",
      "Queue: bin/queue_param/retail.npy\r\n",
      "Output: results/retail/GPU\r\n",
      "People: 2\r\n",
      "Threshold: 0.7\r\n",
      "Model loaded\r\n",
      "Core created\r\n",
      "Network loaded\r\n",
      "Input key: data input shape: [1, 3, 320, 544]\r\n",
      "Output key: detection_out\r\n",
      "Model loaded, loading time: 0:00:36.118535\r\n",
      "Total frames 167, processing time: 0:00:05.499037\r\n",
      "\r\n",
      "########################################################################\r\n",
      "# End of output for job 32112.v-qsvr-1.devcloud-edge\r\n",
      "# Date: Fri Apr 10 11:10:48 PDT 2020\r\n",
      "########################################################################\r\n",
      "\r\n",
      "skipping application metrics\r\n",
      "Your telemetry account is not created\r\n",
      "True\r\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>Results on GPU</h2>\n",
       "    \n",
       "    <video alt=\"\" controls autoplay muted height=\"480\"><source src=\"./results/retail/GPU/out.mp4\" type=\"video/mp4\" /></video>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write your script for Intel Core CPU +GPU video results here\n",
    "!cat RETAIL_GPU.o{job_number_gpu}\n",
    "videoHTML('Results on GPU', ['./results/retail/GPU/out.mp4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "########################################################################\r\n",
      "#      Date:           Fri Apr 10 11:10:00 PDT 2020\r\n",
      "#    Job ID:           32113.v-qsvr-1.devcloud-edge\r\n",
      "#      User:           u40443\r\n",
      "# Resources:           neednodes=1:tank-870:i5-6500te:intel-ncs2,nodes=1:tank-870:i5-6500te:intel-ncs2,walltime=01:00:00\r\n",
      "########################################################################\r\n",
      "\r\n",
      "[setupvars.sh] OpenVINO environment initialized\r\n",
      "Running person_detect.py\r\n",
      "Model: model/person-detection-retail-0013.xml\r\n",
      "Device: MYRIAD\r\n",
      "Video: resources/retail.mp4\r\n",
      "Queue: bin/queue_param/retail.npy\r\n",
      "Output: results/retail/MYRIAD\r\n",
      "People: 2\r\n",
      "Threshold: 0.7\r\n",
      "Model loaded\r\n",
      "Core created\r\n",
      "Network loaded\r\n",
      "Input key: data input shape: [1, 3, 320, 544]\r\n",
      "Output key: detection_out\r\n",
      "Model loaded, loading time: 0:00:02.566522\r\n",
      "Total frames 167, processing time: 0:00:24.505567\r\n",
      "\r\n",
      "########################################################################\r\n",
      "# End of output for job 32113.v-qsvr-1.devcloud-edge\r\n",
      "# Date: Fri Apr 10 11:10:36 PDT 2020\r\n",
      "########################################################################\r\n",
      "\r\n",
      "skipping application metrics\r\n",
      "Your telemetry account is not created\r\n",
      "True\r\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>Results on NCS2</h2>\n",
       "    \n",
       "    <video alt=\"\" controls autoplay muted height=\"480\"><source src=\"./results/retail/MYRIAD/out.mp4\" type=\"video/mp4\" /></video>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write your script for Intel CPU + Intel NCS2 video results here\n",
    "!cat RETAIL_MYRIAD.o{job_number_ncs2}\n",
    "videoHTML('Results on NCS2', ['./results/retail/MYRIAD/out.mp4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "########################################################################\r\n",
      "#      Date:           Fri Apr 10 11:10:02 PDT 2020\r\n",
      "#    Job ID:           32114.v-qsvr-1.devcloud-edge\r\n",
      "#      User:           u40443\r\n",
      "# Resources:           neednodes=1:tank-870:i5-6500te:iei-mustang-f100-a10,nodes=1:tank-870:i5-6500te:iei-mustang-f100-a10,walltime=01:00:00\r\n",
      "########################################################################\r\n",
      "\r\n",
      "[setupvars.sh] OpenVINO environment initialized\r\n",
      "INTELFPGAOCLSDKROOT is set to /opt/altera/aocl-pro-rte/aclrte-linux64. Using that.\r\n",
      "\r\n",
      "aoc was not found, but aocl was found. Assuming only RTE is installed.\r\n",
      "\r\n",
      "AOCL_BOARD_PACKAGE_ROOT is set to /opt/intel/openvino/bitstreams/a10_vision_design_sg1_bitstreams/BSP/a10_1150_sg1. Using that.\r\n",
      "Adding /opt/altera/aocl-pro-rte/aclrte-linux64/bin to PATH\r\n",
      "Adding /opt/altera/aocl-pro-rte/aclrte-linux64/host/linux64/lib to LD_LIBRARY_PATH\r\n",
      "Adding /opt/intel/openvino/bitstreams/a10_vision_design_sg1_bitstreams/BSP/a10_1150_sg1/linux64/lib to LD_LIBRARY_PATH\r\n",
      "[setupvars.sh] OpenVINO environment initialized\r\n",
      "aocl program: Running program from /opt/intel/openvino/bitstreams/a10_vision_design_sg1_bitstreams/BSP/a10_1150_sg1/linux64/libexec\r\n",
      "Programming device: a10gx_2ddr : Intel Vision Accelerator Design with Intel Arria 10 FPGA (acla10_1150_sg10)\r\n",
      "Program succeed. \r\n",
      "Running person_detect.py\r\n",
      "Model: model/person-detection-retail-0013.xml\r\n",
      "Device: HETERO:FPGA,CPU\r\n",
      "Video: resources/retail.mp4\r\n",
      "Queue: bin/queue_param/retail.npy\r\n",
      "Output: results/retail/FPGA\r\n",
      "People: 2\r\n",
      "Threshold: 0.7\r\n",
      "Model loaded\r\n",
      "Core created\r\n",
      "Network loaded\r\n",
      "Input key: data input shape: [1, 3, 320, 544]\r\n",
      "Output key: detection_out\r\n",
      "Model loaded, loading time: 0:00:29.120876\r\n",
      "Total frames 167, processing time: 0:00:03.266186\r\n",
      "\r\n",
      "########################################################################\r\n",
      "# End of output for job 32114.v-qsvr-1.devcloud-edge\r\n",
      "# Date: Fri Apr 10 11:10:50 PDT 2020\r\n",
      "########################################################################\r\n",
      "\r\n",
      "skipping application metrics\r\n",
      "Your telemetry account is not created\r\n",
      "True\r\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>Results on FPGA</h2>\n",
       "    \n",
       "    <video alt=\"\" controls autoplay muted height=\"480\"><source src=\"./results/retail/FPGA/out.mp4\" type=\"video/mp4\" /></video>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write your script for Intel® Arria® 10 FPGA video results here\n",
    "!cat RETAIL_FPGA.o{job_number_fpga}\n",
    "videoHTML('Results on FPGA', ['./results/retail/FPGA/out.mp4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Assess Performance\n",
    "\n",
    "This is where you need to write code to asses how well your model is performing. You will use the `stats.txt` file located in your results directory.\n",
    "You need to compare the following timings for all the models across all 4 devices:\n",
    "\n",
    "- Model loading time\n",
    "- Average Inference Time\n",
    "- FPS\n",
    "\n",
    "Show your results in the form of a bar chart using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device\t Loading time \t   Inference/frame(ms)\t Inference FPS\n",
      "CPU\t 0:00:01.587233\t   27.043\t\t 36.978\n",
      "GPU\t 0:00:36.118535\t   32.928\t\t 30.369\n",
      "MYRIAD\t 0:00:02.566522\t   146.740\t\t 6.815\n",
      "FPGA\t 0:00:29.120876\t   19.558\t\t 51.130\n"
     ]
    }
   ],
   "source": [
    "#TODO Write your code here for model loading time on all 4 device types\n",
    "#TODO Write your code here for model average inference time on all 4 device types\n",
    "#TODO Write your code here for model FPS on all 4 device types\n",
    "\n",
    "def get_stats(device):\n",
    "    output_file = 'results/retail/' + device + '/stats.txt'\n",
    "\n",
    "    load_time = !cat {output_file} | grep 'Model loading time'\n",
    "    ind = len('[\\'Model loading time: ')\n",
    "    load_time = str(load_time)[ind:-2]\n",
    "    \n",
    "    average_frame_time = !cat {output_file} | grep 'Inference time per frame'\n",
    "    ind = len('[\\'Inference time per frame (ms): ')\n",
    "    average_frame_time = str(average_frame_time)[ind:-2]\n",
    "\n",
    "    fps = !cat {output_file} | grep 'Inference FPS: '\n",
    "    ind = len('[\\'Inference FPS: ')\n",
    "    fps = str(fps)[ind:-2]\n",
    "    \n",
    "    return load_time, average_frame_time, fps\n",
    "    \n",
    "\n",
    "load_time_cpu, ave_cpu, fps_cpu = get_stats('CPU')\n",
    "load_time_gpu, ave_gpu, fps_gpu = get_stats('GPU')\n",
    "load_time_ncs2, ave_ncs2, fps_ncs2 = get_stats('MYRIAD')\n",
    "load_time_fpga, ave_fpga, fps_fpga = get_stats('FPGA')\n",
    "\n",
    "print('Device\\t Loading time \\t   Inference/frame(ms)\\t Inference FPS')    \n",
    "print('{}\\t {}\\t   {}\\t\\t {}'.format('CPU', load_time_cpu, ave_cpu, fps_cpu))\n",
    "print('{}\\t {}\\t   {}\\t\\t {}'.format('GPU', load_time_gpu, ave_gpu, fps_gpu))\n",
    "print('{}\\t {}\\t   {}\\t\\t {}'.format('MYRIAD', load_time_ncs2, ave_ncs2, fps_ncs2))\n",
    "print('{}\\t {}\\t   {}\\t\\t {}'.format('FPGA', load_time_fpga, ave_fpga, fps_fpga))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
